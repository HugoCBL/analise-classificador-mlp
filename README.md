# ğŸ§  AnÃ¡lise e OtimizaÃ§Ã£o de um Classificador MLP com Scikit-learn

## ğŸ“Œ DescriÃ§Ã£o do Projeto

Este projeto consiste na implementaÃ§Ã£o, anÃ¡lise e otimizaÃ§Ã£o de uma rede neural artificial do tipo **Multilayer Perceptron (MLP)** para um problema de **classificaÃ§Ã£o binÃ¡ria**.

> Utilizando um conjunto de dados sintÃ©tico gerado pela funÃ§Ã£o `make_classification` do Scikit-learn, o objetivo foi explorar como diferentes hiperparÃ¢metros afetam o desempenho do modelo e, ao final, encontrar uma **arquitetura otimizada** que superasse o modelo base em acurÃ¡cia, mas com menor complexidade.

---

## ğŸ“ ConteÃºdo do RepositÃ³rio

- `Analise e OtimizaÃ§ao de um Classificador MLP com Scikit-learn.ipynb`: Notebook Jupyter/Colab com cÃ³digo para geraÃ§Ã£o de dados, construÃ§Ã£o, treinamento e avaliaÃ§Ã£o dos modelos, alÃ©m do relatÃ³rio final com a anÃ¡lise detalhada.
- `README.md`: Este arquivo, com a documentaÃ§Ã£o do projeto.

---

## ğŸ› ï¸ Tecnologias Utilizadas

- **Python 3**
- **Scikit-learn**
- **NumPy**
- **Matplotlib**
- **Seaborn**

---

## ğŸ“Š Resumo do Projeto e AnÃ¡lises

O projeto foi dividido em **fases de experimentaÃ§Ã£o** para avaliar o impacto de cada hiperparÃ¢metro de forma isolada.

### ğŸ”¹ Modelo Base

ConfiguraÃ§Ã£o inicial da MLP:

- 1 camada escondida com **5 neurÃ´nios**
- FunÃ§Ã£o de ativaÃ§Ã£o: **ReLU**
- Otimizador: **Adam**
- Ã‰pocas: **100**
- **AcurÃ¡cia de ReferÃªncia:** ğŸŸ¢ **86.67%**

---

### ğŸ” AnÃ¡lise de HiperparÃ¢metros

- **NÃºmero de NeurÃ´nios:**  
  Aumentar para 64 neurÃ´nios **nÃ£o melhorou** a performance, indicando que o modelo base jÃ¡ era adequado.

- **NÃºmero de Ã‰pocas:**  
  - 200 Ã©pocas: acurÃ¡cia aumentou para **88.33%**  
  - 50 Ã©pocas: causou **underfitting**, com acurÃ¡cia de **83.33%**

- **Otimizador:**  
  Troca de `adam` por `sgd` levou a pior performance (**81.67%**) e uma fronteira de decisÃ£o quase linear.

- **Profundidade da Rede:**  
  AdiÃ§Ã£o de uma 2Âª camada (`(5, 64)`) tornou o modelo **desnecessariamente complexo**, reduzindo a acurÃ¡cia para **85.00%**

- **FunÃ§Ã£o de AtivaÃ§Ã£o:**  
  `tanh` superou `relu`, alcanÃ§ando **88.33%**

- **Quantidade de Dados:**  
  Aumento de 200 para 1000 amostras teve o **melhor impacto**, com acurÃ¡cia de **89.00%**

---

### ğŸ§ª Desafio de OtimizaÃ§Ã£o: Modelo Final

Objetivo: **Superar o modelo base com menos de 5 neurÃ´nios.**

ğŸ”§ EstratÃ©gia de **ajuste fino**:

- **Arquitetura Final:** 4 neurÃ´nios, ativaÃ§Ã£o `tanh`
- **Ajustes:** taxa de aprendizado = `0.001`, Ã©pocas = `1500`
- âœ… **Resultado Final:** **88.33% de acurÃ¡cia**

> âœ¨ **ConclusÃ£o:** Um modelo mais simples pode ser mais eficaz se for treinado com mais precisÃ£o e paciÃªncia.

---
